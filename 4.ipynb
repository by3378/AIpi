{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3c6d2e1",
   "metadata": {},
   "source": [
    "\n",
    "# Task 4 微调技术（选做）\n",
    "\n",
    "## 前置知识\n",
    "\n",
    "### 监督微调 (SFT)\n",
    "\n",
    "- 明确SFT的目标，并熟悉其典型的数据格式。\n",
    "\n",
    "使用高质量示例数据集，对预训练好的LLM进行微调，以在特定任务上表现更好。\n",
    "```python\n",
    "[\n",
    "  {\"role\": \"user\", \"content\": \"\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"\"}\n",
    "]\n",
    "```\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ec9a5d",
   "metadata": {},
   "source": [
    "\n",
    "### LoRA 技术原理\n",
    "\n",
    "#### 掌握LoRA通过低秩适配来模拟权重更新的核心思想。\n",
    "\n",
    "[LoRA](./LoRA%20Low-Rank%20Adaptation%20of%20Large%20Language%20Models.md)\n",
    "\n",
    "#### 理解其关键超参数的作用及其对模型训练的影响。\n",
    "\n",
    "- $\\alpha,r$：控制更新幅度，类似学习率。\n",
    "- 使用位置：对哪些矩阵进行LoRA微调。\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7027f0e",
   "metadata": {},
   "source": [
    "### 训练优化技术:\n",
    "\n",
    "了解在计算资源有限时进行模型训练的常用技术，如梯度累积、混合精度训练或模型量化的工作原理。\n",
    "\n",
    "- **梯度累积**：将数据分成多个小批次，依次计算每个小批次的梯度并进行累加，达到一定批次后再用累积的总梯度来更新一次模型参数 。使用较小的GPU内存来近似大批次训练。\n",
    "\n",
    "- **混合精度训练**：让模型权重和梯度使用FP32（单精度）以保持数值稳定性，而激活值和中间计算则使用FP16（半精度）来提高计算效率。\n",
    "\n",
    "- **模型量化**：将连续的浮点数值范围映射为离散的整数集。\n",
    "\n",
    "- 层间并行、层内并行、交替稀疏注意力。[GPT3](./Language%20Models%20are%20Few-Shot%20Learners.md)\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb370958",
   "metadata": {},
   "source": [
    "## 实践任务\n",
    "\n",
    "利用 LoRA 技术对 `Qwen/Qwen2.5-0.5B-Instruct` 模型进行监督微调，使其能够准确回答关于《华中科技大学学生手册》的内容。\n",
    "\n",
    "**数据集:** `alleyf/HUST-Student-Handbook` ([数据集](https://www.modelscope.cn/datasets/alleyf/HUST-Student-Handbook/files))，也可以用Task 3自己构建的数据集\n",
    "\n",
    "- 在实际微调中，常会遇到因资源不足导致无法训练的问题。除了选择更小的基础模型外，请至少列举并说明**两种**可以在代码层面实现的、用于缓解显存压力的训练技巧，并简述它们的基本工作原理。\n",
    "\n",
    "答案见上。\n",
    "\n",
    "- 思考应该如何全面、客观地衡量其微调后的效果？尝试设计一个评估方案。\n",
    "  - 双盲测试，由人类评估微调前后对对同一问题的回答质量，包括语言流畅性，与标准答案的一致性等。\n",
    "  - 使用类似数据集，如《武汉大学学生手册》作为验证集，对微调后的模型进行评估。\n",
    "  - 评估其计算资源的消耗。\n",
    "  - 计算标准的NLP指标。如tokenize后的F1 score, Exact Match等\n",
    "```plaintext\n",
    "Precision = 共同词数 / 预测词总数  \n",
    "Recall    = 共同词数 / 标准答案词总数  \n",
    "F1 = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "```\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293149a1",
   "metadata": {},
   "source": [
    "## 数据集\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"conversations\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"用户提问内容\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"助手回答内容\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "### 示例代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c799684",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\modscope\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-09-27 09:57:22,674 - modelscope - WARNING - Use trust_remote_code=True. Will invoke codes from ./data/lora_hust_student_handbookt.jsonl. Please make sure that you can trust the external codes.\n",
      "2025-09-27 09:57:22,674 - modelscope - WARNING - Use trust_remote_code=True. Will invoke codes from json. Please make sure that you can trust the external codes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集包含 719 组问答对\n",
      "{'input': '华中科技大学博士生培养目标中，对学术能力和专业能力分别提出哪些具体要求？', 'output': '华中科技大学博士生培养目标中对学术能力和专业能力的具体要求如下：\\n\\n**学术能力要求**：\\n- 掌握本学科或专业领域坚实宽广的基础理论和系统深入的专门知识；\\n- 具有独立从事科学研究的能力；\\n- 具备良好的写作和国际学术交流能力；\\n- 能够在科学研究领域作出创新性研究。\\n\\n**专业能力要求**：\\n- 具有独立承担专业工作的能力；\\n- 对于专业学位博士生，重点培养其通过专业实践掌握独立承担专业工作的能力。\\n\\n这些要求旨在培养博士生在学术研究和专业实践中的高水平能力，确保他们成为德智体美劳全面发展的社会主义建设者和接班人。'}\n"
     ]
    }
   ],
   "source": [
    "from modelscope.msdatasets import MsDataset\n",
    "\n",
    "dataset = MsDataset.load(\n",
    "    './data/lora_hust_student_handbookt.jsonl',\n",
    "    split='train',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(f\"数据集包含 {len(dataset)} 组问答对\")\n",
    "\n",
    "\n",
    "def prepare_training_data(data):\n",
    "    training_data = []\n",
    "    for item in data:\n",
    "        conversations = item[\"conversations\"]\n",
    "        training_data.append({\n",
    "            \"input\": conversations[0][\"content\"],\n",
    "            \"output\": conversations[1][\"content\"]\n",
    "        })\n",
    "    return training_data\n",
    "\n",
    "training_data = prepare_training_data(dataset)\n",
    "print(training_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5151395e",
   "metadata": {},
   "source": [
    "## 模型\n",
    "\n",
    "### [示例代码](https://www.modelscope.cn/models/qwen/Qwen2.5-0.5B-Instruct/summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5b57e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4c171c",
   "metadata": {},
   "source": [
    "### LoRA 微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edb4b27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: ./qwen\\qwen\\Qwen2.5-0.5B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 16:15:10,830 - modelscope - INFO - Creating symbolic link [./qwen\\qwen\\Qwen2.5-0.5B-Instruct].\n",
      "2025-09-20 16:15:10,832 - modelscope - WARNING - Failed to create symbolic link ./qwen\\qwen\\Qwen2.5-0.5B-Instruct for d:\\2nd\\qwen\\qwen\\Qwen2___5-0___5B-Instruct.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./qwen\\qwen\\Qwen2___5-0___5B-Instruct\n"
     ]
    }
   ],
   "source": [
    "from modelscope import snapshot_download\n",
    "\n",
    "model_dir = snapshot_download('qwen/Qwen2.5-0.5B-Instruct', cache_dir='./qwen')\n",
    "print(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b782ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-27 10:19:17,197 - modelscope - WARNING - Use trust_remote_code=True. Will invoke codes from ./data/lora_hust_student_handbookt.jsonl. Please make sure that you can trust the external codes.\n",
      "2025-09-27 10:19:17,198 - modelscope - WARNING - Use trust_remote_code=True. Will invoke codes from json. Please make sure that you can trust the external codes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集包含 719 组问答对\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='69' max='69' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [69/69 15:23, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>11.518900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>10.052200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>9.031400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from modelscope.msdatasets import MsDataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "\n",
    "\n",
    "dataset = MsDataset.load(\n",
    "    './data/lora_hust_student_handbookt.jsonl',\n",
    "    split='train',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(f\"数据集包含 {len(dataset)} 组问答对\")\n",
    "\n",
    "def prepare_training_data(data):\n",
    "    training_data = []\n",
    "    for item in data:\n",
    "        conversations = item[\"conversations\"]\n",
    "        training_data.append({\n",
    "            \"input\": conversations[0][\"content\"],\n",
    "            \"output\": conversations[1][\"content\"]\n",
    "        })\n",
    "    return training_data\n",
    "data = prepare_training_data(dataset)\n",
    "\n",
    "\n",
    "class SFTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": item[\"input\"]},\n",
    "            {\"role\": \"assistant\", \"content\": item[\"output\"]}\n",
    "        ]\n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        tokenized = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = tokenized[\"input_ids\"].squeeze()  # 去除所有维度为1的轴\n",
    "        attention_mask = tokenized[\"attention_mask\"].squeeze()\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": input_ids}\n",
    "        # \"labels\" 等于 input_ids，即自回归地预测\n",
    "\n",
    "\n",
    "model_dir = './qwen./qwen/Qwen2___5-0___5B-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_dir, trust_remote_code=True)\n",
    "\n",
    "# LoRA配置\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=8,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",           # 不对bias参数进行微调\n",
    "    task_type=\"CAUSAL_LM\"  # 任务类型：因果语言建模（自回归生成）\n",
    ")\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "\n",
    "train_data = SFTDataset(data, tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=4,  # 每块设备上的batch size\n",
    "    gradient_accumulation_steps=8,  # 每8步更新梯度，相当于batch size=32\n",
    "    fp16=True,                      # 混合精度\n",
    "    num_train_epochs=3,\n",
    "    output_dir=\"./lora_output\",\n",
    "    logging_steps=20,               # 每20步记录一次日志\n",
    "    save_steps=100,                 # 每100步保存一次模型\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"./lora_output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8ed681",
   "metadata": {},
   "source": [
    "### 问答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d0723e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "华中科技大学的学生请假流程如下：\n",
      "\n",
      "1. **填写请假单**：首先，你需要填写《华中科技大学学生请假条》，并确保提交给学校相关部门或辅导员。\n",
      "\n",
      "2. **申请请假**：你可以在规定的时间内通过学校的请假系统（如学工网、网上办公平台等）提交你的请假申请。通常需要提供以下信息：\n",
      "   - 姓名和身份证号码。\n",
      "   - 请假事由说明。\n",
      "   - 具体的请假日期。\n",
      "   - 是否有特殊情况需要延长假期。\n",
      "\n",
      "3. **审批过程**：学校会根据实际情况进行审批，包括审核请假原因是否合理，是否有足够的理由来延长假期等。\n",
      "\n",
      "4. **确认并执行**：如果批准了请假请求，你可以按照学校的指示去办理相关手续，并在规定的期限内完成请假。\n",
      "\n",
      "5. **后续处理**：请务必遵守学校的规定，按时完成请假程序，以避免影响其他学生的正常学习生活。\n",
      "\n",
      "请注意，具体的请假流程可能会有所变化，请提前查看最新的校规校纪或者直接联系学校相关部门获取最准确的信息。\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "original_model_dir = './qwen./qwen/Qwen2___5-0___5B-Instruct'\n",
    "finetuned_model_dir = './lora_output'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(  # 因果语言建模的预训练模型\n",
    "    finetuned_model_dir,\n",
    "    torch_dtype=\"auto\",  # 自动选择数据类型\n",
    "    device_map=\"auto\",   # 自动将模型分配到可用的设备\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(original_model_dir, trust_remote_code=True)\n",
    "\n",
    "\n",
    "prompt = \"华中科技大学学生请假流程是什么？\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"你是Qwen，华中科技大学学生手册智能助手。\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(   # 将多轮对话自动格式化为模型需要的输入文本。\n",
    "    messages,\n",
    "    tokenize=False, # 只生成原始文本，不做分词。\n",
    "    add_generation_prompt=True  # 在文本末尾添加生成提示，方便模型生成回复。\n",
    ")  # 类似 <|user|>请假流程是什么？<|assistant|>请假流程如下…… 的普通字符串\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device) # 分词并转换为 PyTorch 张量\n",
    "# model_inputs 字典包含两个 PyTorch 张量\n",
    "# input_ids：分词后的 token 序列\n",
    "# attention_mask：注意力掩码，和inputs_ids 等长\n",
    "\n",
    "\n",
    "generated_ids = model.generate(  # 生成 token 序列\n",
    "    **model_inputs,  # 解包出键值对\n",
    "    max_new_tokens=512  # 最大回复长度\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "] # 从模型的完整输出中截取掉原始输入 prompt\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0] # 一个对话\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modscope",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
