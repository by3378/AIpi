{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0985714c",
   "metadata": {},
   "source": [
    "# Task 3 提示词工程与RAG （选做）\n",
    "\n",
    "## 前置知识\n",
    "\n",
    "### **大语言模型与[提示词工程](https://www.promptingguide.ai/zh):**\n",
    "\t\n",
    "- 理解提示词工程如何引导LLM完成特定任务、生成特定格式输出的技术。\n",
    "- 了解零样本（Zero-shot）、少样本（Few-shot）和思维链（Chain-of-Thought）等主流提示词策略。\n",
    "\n",
    "\n",
    "\n",
    "设计有效的提示词以指导模型执行期望任务的方法被称为提示工程。  \n",
    "零样本提示，即直接提示模型给出一个回答，而没有提供任何示例。类似地有少样本提示。  \n",
    "思维链通过展示中间推理步骤以实现复杂的推理能力。可以与少样本提示结合起来。  \n",
    "零样本提示的思维链，大致就是对模型说“让我们一步一步来思考这个问题”。\n",
    "\n",
    "### [API](./API.ipynb)调用 \n",
    "\n",
    "- 熟悉如何通过API接口与大语言模型进行交互。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0c0776",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### 检索增强生成 (Retrieval-Augmented Generation - [RAG](./Retrieval-Augmented%20Generation%20for%20Knowledge-Intensive%20NLP%20Tasks.md)\n",
    "\n",
    "- 理解RAG如何帮助大模型解决问题。\n",
    "- 掌握基础RAG实现的流程（文档加载、文本分割、构建向量数据库、向量数据库检索、结合检索结果生成回答）。\n",
    "\n",
    "LLM 原始训练数据集之外的新数据称为外部数据。词嵌入将数据存储在向量数据库中。\n",
    "RAG 将用户查询向量与向量数据库匹配，返回高度相关的特定文档。\n",
    "RAG 模型通过在上下文中添加检索到的相关数据来增强用户输入（或提示）\n",
    "需要实时/定期更新外部数据\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032f56ba",
   "metadata": {},
   "source": [
    "\n",
    "## 数据集\n",
    "\n",
    "使用学校的研究生手册作为[数据集](https://gs.hust.edu.cn/info/1137/6338.htm)，完成任务。\n",
    "\n",
    "## 实践任务\n",
    "\n",
    "1. 提示词构建问答数据集\n",
    "\t- 基于提示词从特定文档中自动构建问答（QA）数据集。\n",
    "\t- 设计提示词以明确大模型的任务边界以及输出格式的约束；\n",
    "\t- 理解长文本分割的必要性及策略，以适配大模型的上下文长度限制；\n",
    "\t- 工具支持：可使用阿里云提供的免费 API 调用大模型或者私戳出题人提供。\n",
    "\t\n",
    "2. RAG知识问答\n",
    "\t- 理解检索增强生成（RAG）技术在知识问答中的应用原理与核心环节相关知识。\n",
    "\t\n",
    "\t- 使用API以及研究生手册文档或自主构建的QA数据集实现RAG 的基本流程，使其可以对研究生手册或QA数据集进行知识问答。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c001f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "分块处理进度: 100%|██████████| 178/178 [03:43<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      question                                 answer\n",
      "0    华中科技大学研究生国家奖学金的管理依据是什么文件？                    华中科技大学研究生国家奖学金管理办法。\n",
      "1     哪份文件规定了华中科技大学研究生请假的相关要求？                      华中科技大学关于研究生请假的规定。\n",
      "2  涉及研究生学术道德和学术不端行为处理的文件名称是什么？               华中科技大学学术道德规范及学术不端行为处理规定。\n",
      "3                华中科技大学的校训是什么？                 华中科技大学的校训是“明德厚学，求是创新”。\n",
      "4         目录中关于研究生培养的第一个文件是什么？  目录中关于研究生培养的第一个文件是《华中科技大学博士研究生培养工作规定》。\n",
      "手册未包含相关内容。\n"
     ]
    }
   ],
   "source": [
    "# 构建问答数据集\n",
    "import os\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def extract_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    # with as 是 Python 的上下文管理语法，能确保代码块结束后自动关闭文件，避免资源泄漏。\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            # 如果没有文本内容，则用空字符串代替，避免拼接 None 报错。\n",
    "            text += (page.extract_text() or \"\") + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def extract_chunks(text, max_length=800):\n",
    "    # 按换行符 \\n 分割成一个字符串列表\n",
    "    paragraphs = text.split('\\n')\n",
    "    chunks, chunk = [], \"\"\n",
    "    for para in paragraphs:\n",
    "        if len(chunk) + len(para) < max_length:\n",
    "            chunk += para + \"\\n\"\n",
    "        else:\n",
    "            chunks.append(chunk.strip())\n",
    "            chunk = para + \"\\n\"\n",
    "    if chunk: chunks.append(chunk.strip())  # 最后剩余的内容\n",
    "    return chunks\n",
    "\n",
    "def call_llm(system_prompt, content_prompt):\n",
    "    try:\n",
    "        client = OpenAI(\n",
    "            api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "            base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "        )\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"qwen-plus\",\n",
    "            messages=[\n",
    "                {'role': 'system', 'content': system_prompt},\n",
    "                {'role': 'user', 'content': content_prompt},\n",
    "            ]\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"错误信息：{e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_qa(text):\n",
    "    qa_list, q, a = [], None, None\n",
    "    for line in text.strip().split('\\n'):\n",
    "        if line.startswith(\"问题：\"):\n",
    "            q = line.replace(\"问题：\", \"\").strip()  # 去掉开头的 \"问题：\"\n",
    "        elif line.startswith(\"答案：\"):\n",
    "            a = line.replace(\"答案：\", \"\").strip()\n",
    "            if q and a: qa_list.append({\"question\": q, \"answer\": a})\n",
    "            q, a = None, None\n",
    "    return qa_list\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    content_prompt = f\"请根据以下内容生成3个问答对，格式为：问题：...，答案：...\\n内容：{chunk}\"\n",
    "    result = call_llm(system_prompt, content_prompt)\n",
    "    return extract_qa(result)\n",
    "\n",
    "system_prompt = '你是一个专业的问答（QA）数据集生成助手，请根据输入内容生成3组问题和答案，格式严格按照：问题：...，答案：...'\n",
    "pdf_path = \"./2024研究生手册.pdf\"\n",
    "text = extract_pdf(pdf_path)\n",
    "chunks = extract_chunks(text)\n",
    "qa_list = []\n",
    "# 创建一个线程池，最多同时运行 5 个线程\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    # 用线程池并发地为每个分块提交一个任务，并把所有任务的“未来对象”收集到 futures 列表中。\n",
    "    futures = [executor.submit(process_chunk, chunk) for chunk in chunks]\n",
    "    for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"分块处理进度\"):\n",
    "        qa = future.result()\n",
    "        qa_list.extend(qa)\n",
    "df = pd.DataFrame(qa_list)\n",
    "df.to_csv(\"qa_dataset.csv\", index=False)\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "def get_embedding(text):\n",
    "    # tokenizer 返回 PyTorch 张量\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        return model(**inputs).last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "\n",
    "docs = df['answer'].tolist()\n",
    "doc_embeddings = np.vstack([get_embedding(doc) for doc in docs]).astype('float32')\n",
    "# [回答数, 向量维度]\n",
    "\n",
    "# 用 FAISS 库创建一个基于L2距离的扁平向量索引对象，用于后续的相似度检索。\n",
    "index = faiss.IndexFlatL2(doc_embeddings.shape[1])\n",
    "# 将所有文档的语义向量（doc_embeddings）添加到 FAISS 索引中，建立向量数据库。\n",
    "index.add(doc_embeddings)\n",
    "\n",
    "# 检索函数，用于从向量数据库中找出与查询最相似的文档\n",
    "def retrieve(query, top_k=3):\n",
    "    query_vec = get_embedding(query).reshape(1, -1) # -1 表示自动推断剩余的维度\n",
    "    _, I = index.search(query_vec, top_k)  # 返回距离和索引 [查询数, top_k]\n",
    "    return [docs[i] for i in I[0]]\n",
    "\n",
    "# RAG（检索增强生成）问答函数\n",
    "system_prompt = '你是华中科技大学研究生手册的智能问答助手。请根据下方提供的内容，准确、简明地回答用户的问题。如果内容中没有相关信息，请直接回复“手册未包含相关内容”。'\n",
    "def rag_answer(query):\n",
    "    context = \"\\n\".join(retrieve(query))\n",
    "    content_prompt = f\"回答以下问题：{query}\\n可参考手册内容：{context}\"\n",
    "    return call_llm(system_prompt, content_prompt)\n",
    "\n",
    "\n",
    "print(rag_answer(\"如何申请学位论文答辩？\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfabf24",
   "metadata": {},
   "source": [
    "FAISS（Facebook AI Similarity Search）是由 Facebook AI Research 开发的高效相似性搜索库，主要用于大规模向量检索和聚类。它支持在 CPU 和 GPU 上进行高维向量的快速相似度计算，常用于文本、图像等嵌入向量的最近邻检索（如 RAG、推荐系统、语义搜索等场景）。FAISS 提供多种索引结构，能处理百万级甚至更大规模的数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e92fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检索到的内容： 资格考试通过后，博士生方可申请开题答辩。\n",
      "研究生结业后申请学位的流程须从预答辩环节开始。\n",
      "通过学位论文检测，无需修改即可进入下一环节。\n",
      "手册未包含相关内容。\n",
      "检索到的内容： 直博生和硕博连读生应在第三至第四学期完成资格考试。\n",
      "研究生结业后申请学位的流程须从预答辩环节开始。\n",
      "申请休学一学年的研究生若需提前复学，应在下一学期注册期内提出复学申请，其他时间不办理提前复学。\n",
      "直博生和硕博连读生应在第三至第四学期完成资格考试。\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() # 加载环境变量\n",
    "\n",
    "df = pd.read_csv(\"qa_dataset.csv\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        return model(**inputs).last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "\n",
    "docs = df['answer'].tolist()\n",
    "doc_embeddings = np.vstack([get_embedding(doc) for doc in docs]).astype('float32')\n",
    "\n",
    "index = faiss.IndexFlatL2(doc_embeddings.shape[1])\n",
    "index.add(doc_embeddings)\n",
    "\n",
    "def retrieve(query, top_k=3):\n",
    "    query_vec = get_embedding(query).reshape(1, -1)\n",
    "    _, I = index.search(query_vec, top_k)\n",
    "    return [docs[i] for i in I[0]]\n",
    "\n",
    "system_prompt = (\n",
    "    \"你是华中科技大学研究生手册的智能问答助手。请结合下方内容，尽量准确、简明地回答用户的问题。\"\n",
    "    \"如果确实无法从内容中找到答案，再回复“手册未包含相关内容”。\"\n",
    ")\n",
    "\n",
    "def call_llm(system_prompt, content_prompt):\n",
    "    try:\n",
    "        client = OpenAI(\n",
    "            api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "            base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "        )\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"qwen-plus\",\n",
    "            messages=[\n",
    "                {'role': 'system', 'content': system_prompt},\n",
    "                {'role': 'user', 'content': content_prompt},\n",
    "            ]\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"错误信息：{e}\")\n",
    "        return \"\"\n",
    "\n",
    "def rag_answer(query):\n",
    "    context = \"\\n\".join(retrieve(query))\n",
    "    print(\"检索到的内容：\", context)\n",
    "    content_prompt = f\"回答以下问题：{query}\\n可参考手册内容：{context}\"\n",
    "    return call_llm(system_prompt, content_prompt)\n",
    "\n",
    "\n",
    "print(rag_answer(\"如何申请学位论文答辩？\"))\n",
    "print(rag_answer(\"直博生和硕博连读生应在第几学期完成资格考试？\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "concuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
