{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0985714c",
   "metadata": {},
   "source": [
    "# Task 3 提示词工程与RAG （选做）\n",
    "\n",
    "## 前置知识\n",
    "\n",
    "### **大语言模型与[提示词工程](https://www.promptingguide.ai/zh):**\n",
    "\t\n",
    "- 理解提示词工程如何引导LLM完成特定任务、生成特定格式输出的技术。\n",
    "- 了解零样本（Zero-shot）、少样本（Few-shot）和思维链（Chain-of-Thought）等主流提示词策略。\n",
    "\n",
    "\n",
    "\n",
    "设计有效的提示词以指导模型执行期望任务的方法被称为提示工程。  \n",
    "零样本提示，即直接提示模型给出一个回答，而没有提供任何示例。类似地有少样本提示。  \n",
    "思维链通过展示中间推理步骤以实现复杂的推理能力。可以与少样本提示结合起来。  \n",
    "零样本提示的思维链，大致就是对模型说“让我们一步一步来思考这个问题”。\n",
    "\n",
    "### [API](./API.ipynb)调用 \n",
    "\n",
    "- 熟悉如何通过API接口与大语言模型进行交互。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0c0776",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### 检索增强生成 (Retrieval-Augmented Generation - [RAG](./Retrieval-Augmented%20Generation%20for%20Knowledge-Intensive%20NLP%20Tasks.md)\n",
    "\n",
    "- 理解RAG如何帮助大模型解决问题。\n",
    "- 掌握基础RAG实现的流程（文档加载、文本分割、构建向量数据库、向量数据库检索、结合检索结果生成回答）。\n",
    "\n",
    "LLM 原始训练数据集之外的新数据称为外部数据。词嵌入将数据存储在向量数据库中。\n",
    "RAG 将用户查询向量与向量数据库匹配，返回高度相关的特定文档。\n",
    "RAG 模型通过在上下文中添加检索到的相关数据来增强用户输入（或提示）\n",
    "需要实时/定期更新外部数据\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032f56ba",
   "metadata": {},
   "source": [
    "\n",
    "## 数据集\n",
    "\n",
    "使用学校的研究生手册作为[数据集](https://gs.hust.edu.cn/info/1137/6338.htm)，完成任务。\n",
    "\n",
    "## 实践任务\n",
    "\n",
    "1. 提示词构建问答数据集\n",
    "\t- 基于提示词从特定文档中自动构建问答（QA）数据集。\n",
    "\t- 设计提示词以明确大模型的任务边界以及输出格式的约束；\n",
    "\t- 理解长文本分割的必要性及策略，以适配大模型的上下文长度限制；\n",
    "\t- 工具支持：可使用阿里云提供的免费 API 调用大模型或者私戳出题人提供。\n",
    "\t\n",
    "2. RAG知识问答\n",
    "\t- 理解检索增强生成（RAG）技术在知识问答中的应用原理与核心环节相关知识。\n",
    "\t\n",
    "\t- 使用API以及研究生手册文档或自主构建的QA数据集实现RAG 的基本流程，使其可以对研究生手册或QA数据集进行知识问答。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c001f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "分块处理进度: 100%|██████████| 178/178 [07:09<00:00,  2.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        question  \\\n",
      "0  华中科技大学博士研究生培养工作规定是经哪次会议审议通过的？   \n",
      "1       华中科技大学博士研究生培养工作规定的文号是什么？   \n",
      "2     制定华中科技大学博士研究生培养工作规定的依据有哪些？   \n",
      "3      华中科技大学博士研究生培养工作规定适用于哪些学生？   \n",
      "4        华中科技大学博士研究生的培养目标包括哪些方面？   \n",
      "\n",
      "                                              answer  \n",
      "0           华中科技大学博士研究生培养工作规定是经2023年8月28日校长办公会审议通过的。  \n",
      "1                   华中科技大学博士研究生培养工作规定的文号是校研〔2023〕5号。  \n",
      "2  制定华中科技大学博士研究生培养工作规定的依据是《中华人民共和国学位条例暂行实施办法》（国发〔...  \n",
      "3                            该规定适用于具有华中科技大学学籍的各类博士生。  \n",
      "4                      培养博士生成为德智体美劳全面发展的社会主义建设者和接班人。  \n",
      "申请学位论文答辩的流程如下：\n",
      "\n",
      "1. 完成课程学习和学位论文。\n",
      "2. 接受资格审查，通过后方可进入下一环节。\n",
      "3. 通过学位论文检测。\n",
      "4. 提交学位申请材料。\n",
      "5. 学位论文评审通过。\n",
      "6. 通过上述环节后，方可申请学位论文答辩。\n",
      "\n",
      "注意：若为研究生结业后申请学位，须从预答辩环节开始；涉密论文需经研究生院审定通过后方可撰写；如需重新提交论文，应向研究生院学位办公室重新提交进行检测。所有答辩材料需与本表一并存档。\n"
     ]
    }
   ],
   "source": [
    "# 构建问答数据集\n",
    "import os\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def extract_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    # with as 是 Python 的上下文管理语法，能确保代码块结束后自动关闭文件，避免资源泄漏。\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            # 如果没有文本内容，则用空字符串代替，避免拼接 None 报错。\n",
    "            text += (page.extract_text() or \"\") + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def extract_chunks(text, max_length=800):\n",
    "    # 按换行符 \\n 分割成一个字符串列表\n",
    "    paragraphs = text.split('\\n')\n",
    "    chunks, chunk = [], \"\"\n",
    "    for para in paragraphs:\n",
    "        if len(chunk) + len(para) < max_length:\n",
    "            chunk += para + \"\\n\"\n",
    "        else:\n",
    "            chunks.append(chunk.strip())\n",
    "            chunk = para + \"\\n\"\n",
    "    if chunk: chunks.append(chunk.strip())  # 最后剩余的内容\n",
    "    return chunks\n",
    "\n",
    "def call_llm(system_prompt, content_prompt):\n",
    "    try:\n",
    "        client = OpenAI(\n",
    "            api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "            base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "        )\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"qwen-plus\",\n",
    "            messages=[\n",
    "                {'role': 'system', 'content': system_prompt},\n",
    "                {'role': 'user', 'content': content_prompt},\n",
    "            ]\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"错误信息：{e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_qa(text):\n",
    "    qa_list, q, a = [], None, None\n",
    "    for line in text.strip().split('\\n'):\n",
    "        if line.startswith(\"问题：\"):\n",
    "            q = line.replace(\"问题：\", \"\").strip()  # 去掉开头的 \"问题：\"\n",
    "        elif line.startswith(\"答案：\"):\n",
    "            a = line.replace(\"答案：\", \"\").strip()\n",
    "            if q and a: qa_list.append({\"question\": q, \"answer\": a})\n",
    "            q, a = None, None\n",
    "    return qa_list\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    content_prompt = (\n",
    "        \"你是专业的问答数据集生成助手。请仔细阅读下方内容，尽可能多地生成高质量的问答对。\\n\"\n",
    "        \"要求：\\n\"\n",
    "        \"1. 每个问答对要有实际意义，问题要具体，答案要准确。不要问与页码有关的问题。\\n\"\n",
    "        \"2. 输出格式严格为：问题：...，答案：...\\n\"\n",
    "        \"3. 不要编造内容，答案必须来自给定内容。\\n\"\n",
    "        \"4. 自主根据内容确定生成的问答组数。\\n\"\n",
    "        \"5. 问题和答案之间用换行分隔，不要有多余解释。\\n\"\n",
    "        f\"内容：{chunk}\"\n",
    "    )\n",
    "    result = call_llm(system_prompt, content_prompt)\n",
    "    return extract_qa(result)\n",
    "\n",
    "system_prompt = '你是一个专业的问答（QA）数据集生成助手，请根据输入内容生成3组问题和答案，格式严格按照：问题：...，答案：...'\n",
    "pdf_path = \"./2024研究生手册.pdf\"\n",
    "text = extract_pdf(pdf_path)\n",
    "chunks = extract_chunks(text)\n",
    "qa_list = []\n",
    "# 创建一个线程池，最多同时运行 5 个线程\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    # 用线程池并发地为每个分块提交一个任务，并把所有任务的“未来对象”收集到 futures 列表中。\n",
    "    futures = [executor.submit(process_chunk, chunk) for chunk in chunks]\n",
    "    for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"分块处理进度\"):\n",
    "        qa = future.result()\n",
    "        qa_list.extend(qa)\n",
    "df = pd.DataFrame(qa_list)\n",
    "df.to_csv(\"qa_dataset.csv\", index=False)\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "def get_embedding(text):\n",
    "    # tokenizer 返回 PyTorch 张量\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
    "    with torch.no_grad(): # 取第一个 token 的向量，去掉 batch 维度（因为输入是单条文本）\n",
    "        return model(**inputs).last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "\n",
    "docs = df['answer'].tolist()\n",
    "doc_embeddings = np.vstack([get_embedding(doc) for doc in docs]).astype('float32') # 按行拼接\n",
    "# [回答数, 向量维度]\n",
    "\n",
    "# 用 FAISS 库创建一个基于L2距离的扁平向量索引对象，用于后续的相似度检索。\n",
    "index = faiss.IndexFlatL2(doc_embeddings.shape[1])\n",
    "# 将所有文档的语义向量（doc_embeddings）添加到 FAISS 索引中，建立向量数据库。\n",
    "index.add(doc_embeddings)\n",
    "\n",
    "# 检索函数，用于从向量数据库中找出与查询最相似的文档\n",
    "def retrieve(query, top_k=10):\n",
    "    query_vec = get_embedding(query).reshape(1, -1) # -1 表示自动推断剩余的维度，search() 要求输入是二维数组\n",
    "    _, I = index.search(query_vec, top_k)  # 返回距离和索引 [查询数, top_k]\n",
    "    return [docs[i] for i in I[0]]\n",
    "\n",
    "# RAG（检索增强生成）问答函数\n",
    "system_prompt = '你是华中科技大学研究生手册的智能问答助手。请根据下方提供的内容，准确、简明地回答用户的问题。如果内容中没有相关信息，请直接回复“手册未包含相关内容”。'\n",
    "def rag_answer(query):\n",
    "    context = \"\\n\".join(retrieve(query))\n",
    "    content_prompt = f\"回答以下问题：{query}\\n可参考手册内容：{context}\"\n",
    "    return call_llm(system_prompt, content_prompt)\n",
    "\n",
    "\n",
    "print(rag_answer(\"如何申请学位论文答辩？\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfabf24",
   "metadata": {},
   "source": [
    "FAISS（Facebook AI Similarity Search）是由 Facebook AI Research 开发的高效相似性搜索库，主要用于大规模向量检索和聚类。它支持在 CPU 和 GPU 上进行高维向量的快速相似度计算，常用于文本、图像等嵌入向量的最近邻检索（如 RAG、推荐系统、语义搜索等场景）。FAISS 提供多种索引结构，能处理百万级甚至更大规模的数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6e92fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检索到的内容： 应在全日制硕士最长学习年限内（含休学）完成学业。\n",
      "全日制硕士研究生的最长学习年限为4年（含休学）。\n",
      "2019级及以前的博士生最长学习年限为8年（含休学）。\n",
      "自2020级开始，直博生和专业学位博士生的最长学习年限为7年（含休学）。\n",
      "非全日制硕士研究生的最长学习年限为5年（含休学）。\n",
      "自2020级开始，其他博士生的最长学习年限为6年（含休学）。\n",
      "应限定在最长学习年限内。\n",
      "计入我校读研的学习年限。\n",
      "延期后的毕业时间不得超过相应的最长学习年限（含休学）。\n",
      "办理结业最迟不超过学校规定的最长学习年限，逾期未办理的应予退学处理。\n",
      "现在开始回答：\n",
      "最长修业年限根据学生类型不同而有所区别：\n",
      "\n",
      "- 全日制硕士研究生：4年（含休学）\n",
      "- 非全日制硕士研究生：5年（含休学）\n",
      "- 2019级及以前的博士生：8年（含休学）\n",
      "- 自2020级开始的直博生和专业学位博士生：7年（含休学）\n",
      "- 自2020级开始的其他博士生：6年（含休学）\n",
      "\n",
      "以上均为最长学习年限，含休学时间。\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "df = pd.read_csv(\"qa_dataset.csv\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        return model(**inputs).last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "\n",
    "docs = df['answer'].tolist()\n",
    "doc_embeddings = np.vstack([get_embedding(doc) for doc in docs]).astype('float32')\n",
    "\n",
    "index = faiss.IndexFlatL2(doc_embeddings.shape[1])\n",
    "index.add(doc_embeddings)\n",
    "\n",
    "def retrieve(query, top_k=10):\n",
    "    query_vec = get_embedding(query).reshape(1, -1)\n",
    "    _, I = index.search(query_vec, top_k)\n",
    "    return [docs[i] for i in I[0]]\n",
    "\n",
    "system_prompt = (\n",
    "    \"你是华中科技大学研究生手册的智能问答助手。请结合下方内容，尽量准确、简明地回答用户的问题。\"\n",
    "    \"如果确实无法从内容中找到答案，回复“手册未包含相关内容”。\"\n",
    ")\n",
    "\n",
    "def call_llm(system_prompt, content_prompt):\n",
    "    try:\n",
    "        client = OpenAI(\n",
    "            api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "            base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "        )\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"qwen-plus\",\n",
    "            messages=[\n",
    "                {'role': 'system', 'content': system_prompt},\n",
    "                {'role': 'user', 'content': content_prompt},\n",
    "            ]\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"错误信息：{e}\")\n",
    "        return \"\"\n",
    "\n",
    "def rag_answer(query):\n",
    "    context = \"\\n\".join(retrieve(query))\n",
    "    print(\"检索到的内容：\", context)\n",
    "    print(\"现在开始回答：\")\n",
    "    content_prompt = f\"回答以下问题：{query}\\n可参考手册内容：{context}\"\n",
    "    return call_llm(system_prompt, content_prompt)\n",
    "\n",
    "\n",
    "print(rag_answer(\"最长修业年限是多少？\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "concuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
